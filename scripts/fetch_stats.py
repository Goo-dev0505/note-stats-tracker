"""
noteè¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ—¥æ¬¡å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub Actionsã§æ¯æ—¥å®Ÿè¡Œã—ã€è¨˜äº‹ã”ã¨ã®ãƒ“ãƒ¥ãƒ¼ãƒ»ã‚¹ã‚­ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã‚’CSVã«è“„ç©ã™ã‚‹
"""

import os
import csv
import json
import time
import sys
from datetime import datetime, timezone, timedelta
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
from pathlib import Path

# Windowsç’°å¢ƒã§ã®Unicodeå‡ºåŠ›å¯¾å¿œ
if sys.stdout.encoding and sys.stdout.encoding.lower() not in ("utf-8", "utf8"):
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å®šæ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://note.com"
JST = timezone(timedelta(hours=9))
SCRIPT_DIR = Path(__file__).resolve().parent
DATA_DIR = SCRIPT_DIR.parent / "data"

# CSV ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©ï¼ˆä¸€ã‹æ‰€ã§ç®¡ç†ï¼‰
ARTICLES_HEADER = [
    "date", "note_id", "key", "title",
    "published_at", "created_at", "updated_at",
    "age_days", "read_count", "like_count", "comment_count",
]
SUMMARY_HEADER = [
    "æ—¥ä»˜", "ãƒ“ãƒ¥ãƒ¼åˆè¨ˆ", "ã‚¹ã‚­åˆè¨ˆ", "è¨˜äº‹æ•°",
    "ãƒ“ãƒ¥ãƒ¼/è¨˜äº‹", "ã‚¹ã‚­/è¨˜äº‹", "ã‚¹ã‚­ç‡(%)",
    "ãƒ“ãƒ¥ãƒ¼å‰æ—¥æ¯”(%)", "ã‚¹ã‚­å‰æ—¥æ¯”(%)", "ã‚¹ã‚­ç‡å‰æ—¥æ¯”(%)",
    "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "æ›´æ–°æ™‚åˆ»",
]
FOLLOWERS_HEADER = ["æ—¥ä»˜", "æ™‚åˆ»", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç’°å¢ƒè¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_dotenv():
    """ç°¡æ˜“ .env èª­ã¿è¾¼ã¿ï¼ˆpython-dotenv ä¸è¦ï¼‰"""
    env_path = SCRIPT_DIR.parent / ".env"
    if not env_path.exists():
        print(f"[dotenv] .envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
        return
    print(f"[dotenv] .envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {env_path}")
    loaded = []
    with open(env_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#") or "=" not in line:
                continue
            key, _, value = line.partition("=")
            key = key.strip()
            value = value.strip().strip('"').strip("'")
            if key not in os.environ:
                os.environ[key] = value
                loaded.append(key)
            else:
                print(f"[dotenv] {key}: ç’°å¢ƒå¤‰æ•°ãŒæ—¢ã«è¨­å®šæ¸ˆã¿ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    for key in loaded:
        val = os.environ[key]
        display = f"{val[:20]}...ï¼ˆ{len(val)}æ–‡å­—ï¼‰" if key == "NOTE_COOKIE" else val
        print(f"[dotenv] {key} = {display}")


load_dotenv()

NOTE_COOKIE    = os.environ.get("NOTE_COOKIE", "")
NOTE_USERNAME  = os.environ.get("NOTE_USERNAME", "")
COOKIE_SET_DATE = os.environ.get("COOKIE_SET_DATE", "")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Cookie / èªè¨¼ãƒã‚§ãƒƒã‚¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_today_jst() -> str:
    return datetime.now(JST).strftime("%Y-%m-%d")


def check_cookie_expiry():
    """Cookie ã®æœŸé™ãŒè¿‘ã¥ã„ã¦ã„ãŸã‚‰è­¦å‘Š"""
    if not COOKIE_SET_DATE:
        print("âš  COOKIE_SET_DATE ãŒæœªè¨­å®šã§ã™ã€‚æœŸé™ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    try:
        set_date = datetime.strptime(COOKIE_SET_DATE, "%Y-%m-%d").replace(tzinfo=JST)
        days_elapsed = (datetime.now(JST) - set_date).days
        days_remaining = 90 - days_elapsed
        if days_remaining <= 0:
            print(f"ğŸš¨ CookieãŒæœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆè¨­å®šã‹ã‚‰{days_elapsed}æ—¥çµŒéï¼‰")
        elif days_remaining <= 10:
            print(f"âš  CookieæœŸé™ã¾ã§ã‚ã¨ç´„{days_remaining}æ—¥ã§ã™ï¼æ—©ã‚ã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        else:
            print(f"âœ“ CookieæœŸé™: ã‚ã¨ç´„{days_remaining}æ—¥")
    except ValueError:
        print(f"âš  COOKIE_SET_DATE ã®å½¢å¼ãŒä¸æ­£ã§ã™: {COOKIE_SET_DATE}")


def validate_cookie():
    """Cookie å€¤ã®åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
    if not NOTE_COOKIE:
        sys.exit("ğŸš¨ NOTE_COOKIE ãŒç©ºã§ã™ã€‚.env ã¾ãŸã¯ãƒªãƒã‚¸ãƒˆãƒªã® Secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    if "=" not in NOTE_COOKIE:
        sys.exit(f"ğŸš¨ NOTE_COOKIE ã®å½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆkey=value å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚å…ˆé ­: {NOTE_COOKIE[:30]}")
    if NOTE_COOKIE.startswith("NOTE_COOKIE="):
        sys.exit("ğŸš¨ NOTE_COOKIE ã®å€¤ã« 'NOTE_COOKIE=' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å€¤ã ã‘ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    if len(NOTE_COOKIE) < 50:
        print(f"âš  NOTE_COOKIE ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(NOTE_COOKIE)}æ–‡å­—ï¼‰ã€‚å®Œå…¨ãª Cookie ãƒ˜ãƒƒãƒ€ã‚’ã‚³ãƒ”ãƒ¼ã—ãŸã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print(f"[debug] Cookieå…ˆé ­: {NOTE_COOKIE[:40]}... / {len(NOTE_COOKIE)}æ–‡å­—")


def _make_request(path: str) -> Request:
    req = Request(f"{BASE_URL}{path}")
    req.add_header("Cookie", NOTE_COOKIE)
    req.add_header("User-Agent", "note-stats-tracker")
    return req


def verify_auth():
    """stats API ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‹äº‹å‰ç¢ºèª"""
    print("\nğŸ”‘ èªè¨¼ãƒã‚§ãƒƒã‚¯ä¸­...")
    try:
        with urlopen(_make_request("/api/v1/stats/pv?filter=all&page=1&sort=pv")) as res:
            body = json.loads(res.read().decode("utf-8"))
        if "data" in body and "note_stats" in body["data"]:
            print("âœ“ èªè¨¼OK")
            return
        print("âš  APIã¯å¿œç­”ã—ã¾ã—ãŸãŒ stats ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Cookie ãŒç„¡åŠ¹ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        print(f"  â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼: {list(body.keys())}")
    except HTTPError as e:
        print(f"ğŸš¨ èªè¨¼ãƒã‚§ãƒƒã‚¯å¤±æ•—: HTTP {e.code}")
        if e.code in (401, 403):
            print("  â†’ Cookie ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã® DevTools ã‹ã‚‰ Cookie ãƒ˜ãƒƒãƒ€ã‚’å†å–å¾—ã—ã¦ãã ã•ã„ã€‚")
        try:
            print(f"  â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.read().decode('utf-8')[:200]}")
        except Exception:
            pass
    except URLError as e:
        print(f"âœ— é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e.reason}")
    sys.exit(1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_api(path: str) -> dict:
    """note ã® API ã‚’å‘¼ã¶ã€‚å¤±æ•—æ™‚ã¯ sys.exit"""
    try:
        with urlopen(_make_request(path)) as res:
            return json.loads(res.read().decode("utf-8"))
    except HTTPError as e:
        if e.code in (401, 403):
            sys.exit(f"ğŸš¨ èªè¨¼ã‚¨ãƒ©ãƒ¼({e.code}): Secrets ã® NOTE_COOKIE ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(f"âœ— HTTP ã‚¨ãƒ©ãƒ¼: {e.code}")
    except URLError as e:
        sys.exit(f"âœ— é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e.reason}")


def fetch_all_articles() -> tuple[list[dict], int, int, int]:
    """å…¨è¨˜äº‹ã® stats ã‚’å–å¾—ã— (articles, total_pv, total_like, total_comment) ã‚’è¿”ã™"""
    all_notes: list[dict] = []
    page = 1
    stats: dict = {}

    while True:
        print(f"  ãƒšãƒ¼ã‚¸ {page} å–å¾—ä¸­...")
        data = fetch_api(f"/api/v1/stats/pv?filter=all&page={page}&sort=pv")

        if "data" not in data or "note_stats" not in data["data"]:
            sys.exit("ğŸš¨ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Cookie ãŒç„¡åŠ¹ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        stats = data["data"]
        all_notes.extend(stats["note_stats"])

        if stats.get("last_page", True):
            break
        page += 1
        time.sleep(1)

    total_pv      = stats.get("total_pv", 0)
    total_like    = stats.get("total_like", 0)
    total_comment = stats.get("total_comment", 0)
    print(f"  â†’ {len(all_notes)}è¨˜äº‹å–å¾—å®Œäº†ï¼ˆç·PV: {total_pv}, ç·ã‚¹ã‚­: {total_like}ï¼‰")
    return all_notes, total_pv, total_like, total_comment


def fetch_follower_count() -> int | None:
    """ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—ã€‚NOTE_USERNAME æœªè¨­å®šãªã‚‰ None"""
    if not NOTE_USERNAME:
        print("âš  NOTE_USERNAME ãŒæœªè¨­å®šã§ã™ã€‚ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None
    data = fetch_api(f"/api/v2/creators/{NOTE_USERNAME}")
    count = data.get("data", {}).get("followerCount")
    print(f"  â†’ ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {count}")
    return count


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# v3 æ—¥æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _cache_path() -> Path:
    return DATA_DIR / "v3_dates_cache.json"


def load_dates_cache() -> dict:
    path = _cache_path()
    if not path.exists():
        return {}
    try:
        with open(path, encoding="utf-8") as f:
            raw: dict = json.load(f)
        # æ—§å½¢å¼ï¼ˆå€¤ãŒæ–‡å­—åˆ—ï¼‰ã‚’æ–°å½¢å¼ã«ç§»è¡Œ
        migrated = {}
        for k, v in raw.items():
            if isinstance(v, str):
                migrated[k] = {"published_at": v, "created_at": "", "updated_at": "", "fetched_at": ""}
            else:
                migrated[k] = v
        return migrated
    except (json.JSONDecodeError, OSError):
        print("âš  v3_dates_cache.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
        return {}


def save_dates_cache(cache: dict):
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    with open(_cache_path(), "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


def _is_cache_stale(entry: dict, today_str: str) -> bool:
    fetched_at = entry.get("fetched_at", "")
    if not fetched_at:
        return True
    try:
        return (datetime.strptime(today_str, "%Y-%m-%d") - datetime.strptime(fetched_at, "%Y-%m-%d")).days >= 7
    except ValueError:
        return True


def fetch_note_detail(note_key: str) -> dict:
    """v3 API ã‹ã‚‰è¨˜äº‹ã®æ—¥æ™‚ã‚’å–å¾—ã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºæ–‡å­—ã‚’è¿”ã™ï¼ˆsys.exit ã—ãªã„ï¼‰"""
    try:
        with urlopen(_make_request(f"/api/v3/notes/{note_key}")) as res:
            data = json.loads(res.read().decode("utf-8")).get("data", {})
        published_at = ""
        for key in ("published_at", "publish_at", "first_published_at"):
            if data.get(key):
                published_at = data[key]
                break
        return {
            "published_at": published_at,
            "created_at":   data.get("created_at", ""),
            "updated_at":   data.get("updated_at", ""),
        }
    except (HTTPError, URLError) as e:
        print(f"    âš  v3 API ã‚¨ãƒ©ãƒ¼ ({note_key}): {e}")
        return {"published_at": "", "created_at": "", "updated_at": ""}


def _calc_age_days(today_str: str, published_at: str) -> int | str:
    if not published_at:
        return ""
    try:
        pub_date  = datetime.fromisoformat(published_at).astimezone(JST).date()
        today_date = datetime.strptime(today_str, "%Y-%m-%d").date()
        return (today_date - pub_date).days
    except (ValueError, TypeError):
        return ""


def fetch_note_dates(articles: list[dict], today_str: str) -> list[dict]:
    """å…¨è¨˜äº‹ã®æ—¥æ™‚æƒ…å ±ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ãƒ»7æ—¥ã§å†å–å¾—ï¼‰"""
    cache  = load_dates_cache()
    fetched = 0

    for note in articles:
        note_key = note["key"]
        entry = cache.get(note_key)
        if entry and not _is_cache_stale(entry, today_str):
            note.update({
                "published_at": entry["published_at"],
                "created_at":   entry["created_at"],
                "updated_at":   entry["updated_at"],
            })
        else:
            dates = fetch_note_detail(note_key)
            note.update(dates)
            cache[note_key] = {**dates, "fetched_at": today_str}
            fetched += 1
            if fetched % 10 == 0:
                print(f"    {fetched}ä»¶å–å¾—æ¸ˆã¿...")
            time.sleep(0.2)

        note["age_days"] = _calc_age_days(today_str, note["published_at"])

    cached = len(articles) - fetched
    print(f"  â†’ {len(articles)}è¨˜äº‹ä¸­ {fetched}ä»¶ã‚’ v3 API ã‹ã‚‰å–å¾—ï¼ˆ{cached}ä»¶ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
    save_dates_cache(cache)
    return articles


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV ä¿å­˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _read_csv_keep_except(filepath: Path, skip_date: str, date_col: str) -> tuple[list[list], bool]:
    """
    CSV ã‚’èª­ã¿è¾¼ã¿ã€skip_date ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’é™¤ã„ãŸæ®‹ã‚Šã¨
    ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæœŸå¾…é€šã‚Šã‹ã©ã†ã‹ã‚’è¿”ã™ã€‚
    ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã‘ã‚Œã° ([], False) ã‚’è¿”ã™ã€‚
    """
    if not filepath.exists():
        return [], False
    with open(filepath, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        if date_col not in (reader.fieldnames or []):
            # ãƒ˜ãƒƒãƒ€ãƒ¼ä¸ä¸€è‡´ â†’ æ—§å½¢å¼
            return [], False
        rows = list(reader)

    removed = sum(1 for r in rows if r.get(date_col) == skip_date)
    if removed:
        print(f"  â†’ {skip_date} ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {removed} è¡Œã‚’ä¸Šæ›¸ãã—ã¾ã™")
    kept = [r for r in rows if r.get(date_col) != skip_date]
    return kept, True


def save_articles_csv(today: str, articles: list[dict]):
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ CSV ã«ä¿å­˜ï¼ˆåŒæ—¥ãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãï¼‰"""
    filepath = DATA_DIR / "articles.csv"
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    existing, valid = _read_csv_keep_except(filepath, today, "date")

    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=ARTICLES_HEADER, extrasaction="ignore")
        writer.writeheader()
        # æ—¢å­˜è¡Œã‚’æ›¸ãæˆ»ã™ï¼ˆæ¬ ã‘ã¦ã„ã‚‹ã‚­ãƒ¼ã¯ç©ºæ–‡å­—ã§è£œå®Œï¼‰
        for row in existing:
            writer.writerow({k: row.get(k, "") for k in ARTICLES_HEADER})
        # æ–°ã—ã„è¡Œ
        for note in articles:
            writer.writerow({
                "date":          today,
                "note_id":       note["id"],
                "key":           note["key"],
                "title":         note["name"],
                "published_at":  note.get("published_at", ""),
                "created_at":    note.get("created_at", ""),
                "updated_at":    note.get("updated_at", ""),
                "age_days":      note.get("age_days", ""),
                "read_count":    note["read_count"],
                "like_count":    note["like_count"],
                "comment_count": note.get("comment_count", 0),
            })

    print(f"  â†’ {filepath} ã« {len(articles)} è¡Œæ›¸ãè¾¼ã¿")


def save_daily_summary_csv(
    today: str,
    total_pv: int,
    total_like: int,
    total_comment: int,
    article_count: int,
    follower_count: int | None,
):
    """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ï¼ˆå‰æ—¥æ¯”ã‚‚è‡ªå‹•è¨ˆç®—ã€ãƒ˜ãƒƒãƒ€ãƒ¼ä¸ä¸€è‡´æ™‚ã¯å†æ§‹ç¯‰ï¼‰"""
    filepath = DATA_DIR / "daily_summary.csv"
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    today_slash = today.replace("-", "/")

    # æŒ‡æ¨™è¨ˆç®—
    v_per_a = total_pv  / article_count if article_count > 0 else 0
    l_per_a = total_like / article_count if article_count > 0 else 0
    l_rate  = (total_like / total_pv * 100) if total_pv > 0 else 0
    v_change = l_change = r_change = 0.0

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    existing_rows: list[dict] = []
    if filepath.exists():
        with open(filepath, mode="r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            file_headers = reader.fieldnames or []
            rows = list(reader)

        if "æ—¥ä»˜" in file_headers:
            # å‰æ—¥æ¯”è¨ˆç®—
            non_today = [r for r in rows if r.get("æ—¥ä»˜") != today_slash]
            if non_today:
                last = non_today[-1]
                try:
                    p_v = float(last.get("ãƒ“ãƒ¥ãƒ¼åˆè¨ˆ") or 0)
                    p_l = float(last.get("ã‚¹ã‚­åˆè¨ˆ")   or 0)
                    p_r = float(last.get("ã‚¹ã‚­ç‡(%)")  or 0)
                    if p_v > 0: v_change = (total_pv   - p_v) / p_v * 100
                    if p_l > 0: l_change = (total_like - p_l) / p_l * 100
                    if p_r > 0: r_change = (l_rate     - p_r) / p_r * 100
                except (ValueError, TypeError):
                    pass
            existing_rows = non_today
        else:
            # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ â†’ ç ´æ£„ã—ã¦æ–°å½¢å¼ã«ç§»è¡Œ
            print("  âš  daily_summary.csv ãŒæ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãŸã‚æ–°å½¢å¼ã«ç§»è¡Œã—ã¾ã™")
            existing_rows = []

    new_row = {
        "æ—¥ä»˜":            today_slash,
        "ãƒ“ãƒ¥ãƒ¼åˆè¨ˆ":       total_pv,
        "ã‚¹ã‚­åˆè¨ˆ":         total_like,
        "è¨˜äº‹æ•°":           article_count,
        "ãƒ“ãƒ¥ãƒ¼/è¨˜äº‹":      round(v_per_a, 2),
        "ã‚¹ã‚­/è¨˜äº‹":        round(l_per_a, 2),
        "ã‚¹ã‚­ç‡(%)":        round(l_rate, 2),
        "ãƒ“ãƒ¥ãƒ¼å‰æ—¥æ¯”(%)":  round(v_change, 2),
        "ã‚¹ã‚­å‰æ—¥æ¯”(%)":    round(l_change, 2),
        "ã‚¹ã‚­ç‡å‰æ—¥æ¯”(%)":  round(r_change, 2),
        "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°":     follower_count if follower_count is not None else "",
        "æ›´æ–°æ™‚åˆ»":         datetime.now(JST).strftime("%H:%M:%S"),
    }

    with open(filepath, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=SUMMARY_HEADER, extrasaction="ignore")
        writer.writeheader()
        for r in existing_rows:
            writer.writerow({k: r.get(k, "") for k in SUMMARY_HEADER})
        writer.writerow(new_row)

    print(f"  â†’ {filepath} ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆ{today_slash}ï¼‰")


def save_followers_csv(follower_count: int | None):
    """
    ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒå‰å›ã‹ã‚‰å¤‰åŒ–ã—ãŸã¨ãã ã‘1è¡Œè¿½åŠ ã™ã‚‹ã€‚
    å¤‰åŒ–ãªã— â†’ ã‚¹ã‚­ãƒƒãƒ—ã€å–å¾—å¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    if follower_count is None:
        print("  âš  ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãŒå–å¾—ã§ããªã‹ã£ãŸã®ã§ã‚¹ã‚­ãƒƒãƒ—")
        return

    filepath = DATA_DIR / "followers.csv"
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    now_jst   = datetime.now(JST)
    date_str  = now_jst.strftime("%Y/%m/%d")
    time_str  = now_jst.strftime("%H:%M:%S")

    # ç›´è¿‘ã®ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’ç¢ºèª
    last_count: int | None = None
    if filepath.exists():
        with open(filepath, newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°" in (reader.fieldnames or []):
                rows = list(reader)
                for row in reversed(rows):
                    val = row.get("ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "").strip()
                    if val:
                        try:
                            last_count = int(val.replace(",", ""))
                        except ValueError:
                            pass
                        break

    if last_count == follower_count:
        print(f"  ğŸŸ° ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å¤‰åŒ–ãªã—ï¼ˆ{follower_count}ï¼‰ã€‚æ›¸ãè¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—")
        return

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã‘ã‚Œã°æ–°è¦ä½œæˆã€ã‚ã‚Œã°è¿½è¨˜
    write_header = not filepath.exists() or filepath.stat().st_size == 0
    with open(filepath, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=FOLLOWERS_HEADER)
        if write_header:
            writer.writeheader()
        writer.writerow({
            "æ—¥ä»˜":       date_str,
            "æ™‚åˆ»":       time_str,
            "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°": follower_count,
        })

    prev_str = str(last_count) if last_count is not None else "ä¸æ˜"
    print(f"  âœ… ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¤‰åŒ–ã‚’æ¤œçŸ¥ â†’ è¿½è¨˜: {date_str} {time_str} {follower_count}ï¼ˆå‰å›: {prev_str}ï¼‰")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¡ã‚¤ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("=== note-stats-tracker ===")
    today = get_today_jst()
    print(f"æ—¥ä»˜: {today}")

    validate_cookie()
    check_cookie_expiry()
    verify_auth()

    print("\nğŸ“Š è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    articles, total_pv, total_like, total_comment = fetch_all_articles()

    print("\nğŸ“… æ—¥æ™‚æƒ…å ±ï¼ˆpublished_atç­‰ï¼‰å–å¾—ä¸­...")
    articles = fetch_note_dates(articles, today)

    print("\nğŸ‘¥ ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—ä¸­...")
    follower_count = fetch_follower_count()

    DATA_DIR.mkdir(parents=True, exist_ok=True)

    print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    save_articles_csv(today, articles)
    save_daily_summary_csv(today, total_pv, total_like, total_comment, len(articles), follower_count)
    save_followers_csv(follower_count)

    print(f"\n=== å®Œäº† ===")
    print(f"è¨˜äº‹æ•°:       {len(articles)}")
    print(f"ç·PV:         {total_pv}")
    print(f"ç·ã‚¹ã‚­:       {total_like}")
    print(f"ç·ã‚³ãƒ¡ãƒ³ãƒˆ:   {total_comment}")
    if follower_count is not None:
        print(f"ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼:   {follower_count}")


if __name__ == "__main__":
    main()
